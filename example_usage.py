#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Oracleå½’æ¡£æ—¥å¿—åˆ†æ®µå¤„ç†å™¨ä½¿ç”¨ç¤ºä¾‹

è¿™ä¸ªç¤ºä¾‹å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨æ™ºèƒ½åˆ†æ®µç®—æ³•æ¥å¹¶è¡Œå¤„ç†Oracleå½’æ¡£æ—¥å¿—ï¼Œ
æé«˜å¤§æ‰¹é‡æ•°æ®å¤„ç†çš„æ€§èƒ½ã€‚
"""

import cx_Oracle
import json
import time
from oracle_log_segmentation import (
    OracleLogSegmentProcessor, 
    SegmentationConfig
)


def create_oracle_connection():
    """åˆ›å»ºOracleæ•°æ®åº“è¿žæŽ¥"""
    # è¯·æ ¹æ®å®žé™…çŽ¯å¢ƒä¿®æ”¹è¿žæŽ¥å‚æ•°
    dsn = cx_Oracle.makedsn(
        host='localhost',
        port=1521,
        service_name='ORCL'
    )
    
    connection = cx_Oracle.connect(
        user='your_username',
        password='your_password',
        dsn=dsn
    )
    
    return connection


def setup_logminer_sql(archive_log_file: str) -> str:
    """è®¾ç½®LogMinerçš„SQLè¯­å¥"""
    return f"""
    BEGIN
        -- æ·»åŠ å½’æ¡£æ—¥å¿—æ–‡ä»¶
        DBMS_LOGMNR.ADD_LOGFILE(
            LOGFILENAME => '{archive_log_file}',
            OPTIONS => DBMS_LOGMNR.NEW
        );
        
        -- å¯åŠ¨LogMiner
        DBMS_LOGMNR.START_LOGMNR(
            OPTIONS => DBMS_LOGMNR.SKIP_CORRUPTION + 
                      DBMS_LOGMNR.NO_SQL_DELIMITER + 
                      DBMS_LOGMNR.NO_ROWID_IN_STMT
        );
    END;
    """


def my_record_processor(records):
    """
    è‡ªå®šä¹‰çš„è®°å½•å¤„ç†å‡½æ•°
    
    Args:
        records: è®°å½•åˆ—è¡¨ï¼Œæ¯æ¡è®°å½•åŒ…å«scn, timestamp, operationç­‰å­—æ®µ
    """
    for record in records:
        # è¿™é‡Œå®žçŽ°æ‚¨çš„ä¸šåŠ¡é€»è¾‘
        # ä¾‹å¦‚ï¼šæ•°æ®è½¬æ¢ã€å†™å…¥ç›®æ ‡ç³»ç»Ÿã€è§¦å‘ä¸šåŠ¡è§„åˆ™ç­‰
        
        if record['operation'] in ['INSERT', 'UPDATE', 'DELETE']:
            print(f"å¤„ç†æ“ä½œ: {record['operation']} on {record['table_name']} "
                  f"at SCN {record['scn']}")
            
            # ç¤ºä¾‹ï¼šå°†é‡è¦å˜æ›´å†™å…¥æ–‡ä»¶
            if record['table_name'] in ['IMPORTANT_TABLE1', 'IMPORTANT_TABLE2']:
                with open(f"/tmp/changes_{record['table_name']}.log", "a") as f:
                    f.write(f"{record['scn']},{record['timestamp']},{record['sql_redo']}\n")


def example_basic_usage():
    """åŸºç¡€ä½¿ç”¨ç¤ºä¾‹"""
    print("=== åŸºç¡€ä½¿ç”¨ç¤ºä¾‹ ===")
    
    # 1. é…ç½®åˆ†æ®µå‚æ•°
    config = SegmentationConfig(
        target_records_per_segment=50000,  # æ¯æ®µç›®æ ‡5ä¸‡æ¡è®°å½•
        max_segments=10,                    # æœ€å¤š10ä¸ªåˆ†æ®µ
        min_segments=2,                     # æœ€å°‘2ä¸ªåˆ†æ®µ
        sampling_ratio=0.002,               # 0.2%é‡‡æ ·çŽ‡
        tolerance_ratio=0.2                 # 20%å®¹å·®
    )
    
    # 2. åˆ›å»ºå¤„ç†å™¨
    processor = OracleLogSegmentProcessor(
        connection_factory=create_oracle_connection,
        logminer_setup_sql=setup_logminer_sql('/path/to/archive_log.arc'),
        config=config
    )
    
    # 3. å¹¶è¡Œå¤„ç†å½’æ¡£æ—¥å¿—
    start_scn = 100
    end_scn = 10000
    
    print(f"å¼€å§‹å¤„ç†SCNèŒƒå›´: {start_scn} - {end_scn}")
    
    stats = processor.process_archive_log_parallel(
        start_scn=start_scn,
        end_scn=end_scn,
        record_processor=my_record_processor,
        max_workers=4  # ä½¿ç”¨4ä¸ªå¹¶è¡Œçº¿ç¨‹
    )
    
    # 4. æ˜¾ç¤ºå¤„ç†ç»“æžœ
    print("\nå¤„ç†ç»Ÿè®¡ä¿¡æ¯:")
    print(f"æ€»åˆ†æ®µæ•°: {stats['total_segments']}")
    print(f"æ€»è®°å½•æ•°: {stats['total_records']}")
    print(f"æ€»è€—æ—¶: {stats['total_time']:.2f}ç§’")
    print(f"å¹³å‡é€Ÿåº¦: {stats['records_per_second']:.1f}æ¡/ç§’")
    
    print("\nå„åˆ†æ®µè¯¦æƒ…:")
    for i, segment in enumerate(stats['segment_details']):
        accuracy = (1 - segment['accuracy']) * 100
        print(f"åˆ†æ®µ{i+1}: {segment['range']} - "
              f"ä¼°ç®—{segment['estimated_records']}æ¡, "
              f"å®žé™…{segment['actual_records']}æ¡, "
              f"å‡†ç¡®çŽ‡{accuracy:.1f}%, "
              f"è€—æ—¶{segment['processing_time']:.2f}ç§’")


def example_high_performance_usage():
    """é«˜æ€§èƒ½å¤„ç†ç¤ºä¾‹"""
    print("\n=== é«˜æ€§èƒ½å¤„ç†ç¤ºä¾‹ ===")
    
    # é’ˆå¯¹å¤§æ•°æ®é‡ä¼˜åŒ–çš„é…ç½®
    config = SegmentationConfig(
        target_records_per_segment=100000,  # æ¯æ®µ10ä¸‡æ¡è®°å½•
        max_segments=16,                    # æœ€å¤š16ä¸ªåˆ†æ®µ
        min_segments=4,                     # æœ€å°‘4ä¸ªåˆ†æ®µ
        sampling_ratio=0.001,               # 0.1%é‡‡æ ·çŽ‡ï¼ˆå‡å°‘é‡‡æ ·å¼€é”€ï¼‰
        max_sampling_records=5000,          # æœ€å¤šé‡‡æ ·5000æ¡
        tolerance_ratio=0.25                # 25%å®¹å·®ï¼ˆå…è®¸æ›´å¤§çš„ä¸å‡åŒ€æ€§ï¼‰
    )
    
    # è‡ªå®šä¹‰é«˜æ€§èƒ½è®°å½•å¤„ç†å™¨
    processed_count = 0
    batch_buffer = []
    
    def high_performance_processor(records):
        nonlocal processed_count, batch_buffer
        
        # æ‰¹é‡ç¼“å­˜å¤„ç†ä»¥å‡å°‘I/O
        batch_buffer.extend(records)
        processed_count += len(records)
        
        # æ¯10ä¸‡æ¡è®°å½•æ‰¹é‡å†™å…¥ä¸€æ¬¡
        if len(batch_buffer) >= 100000:
            # è¿™é‡Œå¯ä»¥æ‰¹é‡å†™å…¥æ•°æ®åº“ã€æ¶ˆæ¯é˜Ÿåˆ—ç­‰
            print(f"æ‰¹é‡å¤„ç†äº† {len(batch_buffer)} æ¡è®°å½•ï¼Œç´¯è®¡ {processed_count} æ¡")
            batch_buffer.clear()
    
    processor = OracleLogSegmentProcessor(
        connection_factory=create_oracle_connection,
        logminer_setup_sql=setup_logminer_sql('/path/to/large_archive_log.arc'),
        config=config
    )
    
    # å¤„ç†å¤§èŒƒå›´SCN
    start_scn = 1000
    end_scn = 100000
    
    print(f"å¼€å§‹é«˜æ€§èƒ½å¤„ç†SCNèŒƒå›´: {start_scn} - {end_scn}")
    
    start_time = time.time()
    stats = processor.process_archive_log_parallel(
        start_scn=start_scn,
        end_scn=end_scn,
        record_processor=high_performance_processor,
        max_workers=8  # ä½¿ç”¨8ä¸ªå¹¶è¡Œçº¿ç¨‹
    )
    
    # å¤„ç†å‰©ä½™ç¼“å­˜
    if batch_buffer:
        print(f"å¤„ç†å‰©ä½™ {len(batch_buffer)} æ¡è®°å½•")
        batch_buffer.clear()
    
    total_time = time.time() - start_time
    print(f"\né«˜æ€§èƒ½å¤„ç†å®Œæˆ:")
    print(f"æ€»å¤„ç†æ—¶é—´: {total_time:.2f}ç§’")
    print(f"å¤„ç†é€Ÿåº¦: {stats['records_per_second']:.1f}æ¡/ç§’")
    print(f"åˆ†æ®µæ•ˆçŽ‡: å¹³å‡æ¯æ®µ{stats['avg_time_per_segment']:.2f}ç§’")


def example_custom_segmentation():
    """è‡ªå®šä¹‰åˆ†æ®µç­–ç•¥ç¤ºä¾‹"""
    print("\n=== è‡ªå®šä¹‰åˆ†æ®µç­–ç•¥ç¤ºä¾‹ ===")
    
    # é’ˆå¯¹ç‰¹å®šä¸šåŠ¡åœºæ™¯çš„åˆ†æ®µé…ç½®
    config = SegmentationConfig(
        target_records_per_segment=30000,   # è¾ƒå°çš„åˆ†æ®µä»¥ç¡®ä¿å“åº”æ€§
        max_segments=20,                    # å…è®¸æ›´å¤šåˆ†æ®µ
        min_segments=3,                     
        sampling_ratio=0.005,               # æ›´é«˜çš„é‡‡æ ·çŽ‡ä»¥æé«˜å‡†ç¡®æ€§
        tolerance_ratio=0.15                # æ›´ä¸¥æ ¼çš„å®¹å·®
    )
    
    # è‡ªå®šä¹‰å¤„ç†å™¨ï¼Œæ”¯æŒç‰¹å®šè¡¨çš„ç‰¹æ®Šå¤„ç†
    critical_tables = {'USER_ACCOUNTS', 'TRANSACTIONS', 'AUDIT_LOG'}
    
    def custom_processor(records):
        critical_changes = []
        normal_changes = []
        
        for record in records:
            if record['table_name'] in critical_tables:
                critical_changes.append(record)
            else:
                normal_changes.append(record)
        
        # ä¼˜å…ˆå¤„ç†å…³é”®è¡¨çš„å˜æ›´
        if critical_changes:
            print(f"å¤„ç† {len(critical_changes)} æ¡å…³é”®è¡¨å˜æ›´")
            # å®žæ—¶å¤„ç†å…³é”®å˜æ›´
            for change in critical_changes:
                # å‘é€å‘Šè­¦ã€å®žæ—¶åŒæ­¥ç­‰
                pass
        
        if normal_changes:
            print(f"æ‰¹é‡å¤„ç† {len(normal_changes)} æ¡æ™®é€šå˜æ›´")
            # æ‰¹é‡å¤„ç†æ™®é€šå˜æ›´
    
    processor = OracleLogSegmentProcessor(
        connection_factory=create_oracle_connection,
        logminer_setup_sql=setup_logminer_sql('/path/to/business_archive_log.arc'),
        config=config
    )
    
    stats = processor.process_archive_log_parallel(
        start_scn=5000,
        end_scn=50000,
        record_processor=custom_processor,
        max_workers=6
    )
    
    print(f"è‡ªå®šä¹‰å¤„ç†å®Œæˆ: {stats['total_records']}æ¡è®°å½•")


def example_monitoring_and_alerting():
    """ç›‘æŽ§å’Œå‘Šè­¦ç¤ºä¾‹"""
    print("\n=== ç›‘æŽ§å’Œå‘Šè­¦ç¤ºä¾‹ ===")
    
    config = SegmentationConfig(
        target_records_per_segment=80000,
        max_segments=12
    )
    
    # ç›‘æŽ§å¤„ç†è¿›åº¦
    def monitoring_processor(records):
        # ç›‘æŽ§å¤„ç†é€Ÿåº¦
        current_time = time.time()
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å¼‚å¸¸æ“ä½œ
        for record in records:
            if record['operation'] == 'DELETE' and 'CRITICAL' in record['table_name']:
                print(f"âš ï¸  å‘Šè­¦: å…³é”®è¡¨åˆ é™¤æ“ä½œ - {record['table_name']} at SCN {record['scn']}")
            
            if 'DROP' in (record['sql_redo'] or ''):
                print(f"ðŸš¨ ä¸¥é‡å‘Šè­¦: DDLæ“ä½œæ£€æµ‹ - {record['sql_redo'][:100]}...")
    
    processor = OracleLogSegmentProcessor(
        connection_factory=create_oracle_connection,
        logminer_setup_sql=setup_logminer_sql('/path/to/monitored_log.arc'),
        config=config
    )
    
    # å¤„ç†å¹¶ç›‘æŽ§
    stats = processor.process_archive_log_parallel(
        start_scn=2000,
        end_scn=20000,
        record_processor=monitoring_processor,
        max_workers=4
    )
    
    # ç”Ÿæˆç›‘æŽ§æŠ¥å‘Š
    report = {
        'timestamp': time.time(),
        'performance': stats,
        'status': 'completed',
        'segments_accuracy': [
            segment['accuracy'] for segment in stats['segment_details']
        ]
    }
    
    with open('/tmp/processing_report.json', 'w') as f:
        json.dump(report, f, indent=2, default=str)
    
    print("ç›‘æŽ§æŠ¥å‘Šå·²ä¿å­˜åˆ° /tmp/processing_report.json")


if __name__ == "__main__":
    print("Oracleå½’æ¡£æ—¥å¿—æ™ºèƒ½åˆ†æ®µå¤„ç†å™¨ç¤ºä¾‹\n")
    
    try:
        # è¿è¡Œå„ç§ä½¿ç”¨ç¤ºä¾‹
        example_basic_usage()
        example_high_performance_usage()
        example_custom_segmentation()
        example_monitoring_and_alerting()
        
        print("\nâœ… æ‰€æœ‰ç¤ºä¾‹æ‰§è¡Œå®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ æ‰§è¡Œé”™è¯¯: {e}")
        print("è¯·æ£€æŸ¥Oracleè¿žæŽ¥é…ç½®å’ŒLogMineræƒé™")


# é¢å¤–å·¥å…·å‡½æ•°

def analyze_archive_log_scn_range(archive_log_file: str) -> tuple:
    """åˆ†æžå½’æ¡£æ—¥å¿—çš„SCNèŒƒå›´"""
    try:
        conn = create_oracle_connection()
        cursor = conn.cursor()
        
        # è®¾ç½®LogMiner
        setup_sql = setup_logminer_sql(archive_log_file)
        cursor.execute(setup_sql)
        
        # æŸ¥è¯¢SCNèŒƒå›´
        cursor.execute("""
            SELECT MIN(scn) as min_scn, MAX(scn) as max_scn, COUNT(*) as total_records
            FROM v$logmnr_contents
        """)
        
        result = cursor.fetchone()
        min_scn, max_scn, total_records = result
        
        cursor.close()
        conn.close()
        
        print(f"å½’æ¡£æ—¥å¿—åˆ†æžç»“æžœ:")
        print(f"  SCNèŒƒå›´: {min_scn} - {max_scn}")
        print(f"  æ€»è®°å½•æ•°: {total_records}")
        print(f"  SCNè·¨åº¦: {max_scn - min_scn}")
        print(f"  å¹³å‡å¯†åº¦: {total_records / (max_scn - min_scn):.4f} è®°å½•/SCN")
        
        return min_scn, max_scn, total_records
        
    except Exception as e:
        print(f"åˆ†æžå½’æ¡£æ—¥å¿—å¤±è´¥: {e}")
        return None, None, None


def estimate_optimal_segments(total_records: int, target_per_segment: int = 100000) -> int:
    """ä¼°ç®—æœ€ä¼˜åˆ†æ®µæ•°"""
    if total_records <= target_per_segment:
        return 1
    
    optimal_segments = max(2, min(20, total_records // target_per_segment))
    print(f"åŸºäºŽ{total_records}æ¡è®°å½•ï¼Œå»ºè®®åˆ†æˆ{optimal_segments}ä¸ªæ®µ")
    return optimal_segments