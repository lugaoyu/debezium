[![License](http://img.shields.io/:license-apache%202.0-brightgreen.svg)](http://www.apache.org/licenses/LICENSE-2.0.html)
[![Maven Central](https://maven-badges.herokuapp.com/maven-central/io.debezium/debezium-parent/badge.svg)](http://search.maven.org/#search%7Cga%7C1%7Cg%3A%22io.debezium%22)
[![User chat](https://img.shields.io/badge/chat-users-brightgreen.svg)](https://debezium.zulipchat.com/#narrow/stream/302529-users)
[![Developer chat](https://img.shields.io/badge/chat-devs-brightgreen.svg)](https://debezium.zulipchat.com/#narrow/stream/302533-dev)
[![Google Group](https://img.shields.io/:mailing%20list-debezium-brightgreen.svg)](https://groups.google.com/forum/#!forum/debezium)
[![Stack Overflow](http://img.shields.io/:stack%20overflow-debezium-brightgreen.svg)](http://stackoverflow.com/questions/tagged/debezium)

Copyright Debezium Authors.
Licensed under the [Apache License, Version 2.0](http://www.apache.org/licenses/LICENSE-2.0).
The Antlr grammars within the debezium-ddl-parser module are licensed under the [MIT License](https://opensource.org/licenses/MIT).

English | [Chinese](README_ZH.md) | [Japanese](README_JA.md) | [Korean](README_KO.md)

# Debezium

Debezium is an open source project that provides a low latency data streaming platform for change data capture (CDC). You set up and configure Debezium to monitor your databases, and then your applications consume events for each row-level change made to the database. Only committed changes are visible, so your application doesn't have to worry about transactions or changes that are rolled back. Debezium provides a single model of all change events, so your application does not have to worry about the intricacies of each kind of database management system. Additionally, since Debezium records the history of data changes in durable, replicated logs, your application can be stopped and restarted at any time, and it will be able to consume all of the events it missed while it was not running, ensuring that all events are processed correctly and completely.

Monitoring databases and being notified when data changes has always been complicated. Relational database triggers can be useful, but are specific to each database and often limited to updating state within the same database (not communicating with external processes). Some databases offer APIs or frameworks for monitoring changes, but there is no standard so each database's approach is different and requires a lot of knowledged and specialized code. It still is very challenging to ensure that all changes are seen and processed in the same order while minimally impacting the database.

Debezium provides modules that do this work for you. Some modules are generic and work with multiple database management systems, but are also a bit more limited in functionality and performance. Other modules are tailored for specific database management systems, so they are often far more capable and they leverage the specific features of the system.

## Basic architecture

Debezium is a change data capture (CDC) platform that achieves its durability, reliability, and fault tolerance qualities by reusing Kafka and Kafka Connect. Each connector deployed to the Kafka Connect distributed, scalable, fault tolerant service monitors a single upstream database server, capturing all of the changes and recording them in one or more Kafka topics (typically one topic per database table). Kafka ensures that all of these data change events are replicated and totally ordered, and allows many clients to independently consume these same data change events with little impact on the upstream system. Additionally, clients can stop consuming at any time, and when they restart they resume exactly where they left off. Each client can determine whether they want exactly-once or at-least-once delivery of all data change events, and all data change events for each database/table are delivered in the same order they occurred in the upstream database.

Applications that don't need or want this level of fault tolerance, performance, scalability, and reliability can instead use Debezium's *embedded connector engine* to run a connector directly within the application space. They still want the same data change events, but prefer to have the connectors send them directly to the application rather than persist them inside Kafka.

## Common use cases

There are a number of scenarios in which Debezium can be extremely valuable, but here we outline just a few of them that are more common.

### Cache invalidation

Automatically invalidate entries in a cache as soon as the record(s) for entries change or are removed. If the cache is running in a separate process (e.g., Redis, Memcache, Infinispan, and others), then the simple cache invalidation logic can be placed into a separate process or service, simplifying the main application. In some situations, the logic can be made a little more sophisticated and can use the updated data in the change events to update the affected cache entries.

### Simplifying monolithic applications

Many applications update a database and then do additional work after the changes are committed: update search indexes, update a cache, send notifications, run business logic, etc. This is often called "dual-writes" since the application is writing to multiple systems outside of a single transaction. Not only is the application logic complex and more difficult to maintain, dual writes also risk losing data or making the various systems inconsistent if the application were to crash after a commit but before some/all of the other updates were performed. Using change data capture, these other activities can be performed in separate threads or separate processes/services when the data is committed in the original database. This approach is more tolerant of failures, does not miss events, scales better, and more easily supports upgrading and operations.

### Sharing databases

When multiple applications share a single database, it is often non-trivial for one application to become aware of the changes committed by another application. One approach is to use a message bus, although non-transactional message busses suffer from the "dual-writes" problems mentioned above. However, this becomes very straightforward with Debezium: each application can monitor the database and react to the changes.

### Data integration

Data is often stored in multiple places, especially when it is used for different purposes and has slightly different forms. Keeping the multiple systems synchronized can be challenging, but simple ETL-type solutions can be implemented quickly with Debezium and simple event processing logic.

### CQRS

The [Command Query Responsibility Separation (CQRS)](http://martinfowler.com/bliki/CQRS.html) architectural pattern uses a one data model for updating and one or more other data models for reading. As changes are recorded on the update-side, those changes are then processed and used to update the various read representations. As a result CQRS applications are usually more complicated, especially when they need to ensure reliable and totally-ordered processing. Debezium and CDC can make this more approachable: writes are recorded as normal, but Debezium captures those changes in durable, totally ordered streams that are consumed by the services that asynchronously update the read-only views. The write-side tables can represent domain-oriented entities, or when CQRS is paired with [Event Sourcing](http://martinfowler.com/eaaDev/EventSourcing.html) the write-side tables are the append-only event log of commands.

## Building Debezium

The following software is required to work with the Debezium codebase and build it locally:

* [Git](https://git-scm.com) 2.2.1 or later
* JDK 21 or later, e.g. [OpenJDK](http://openjdk.java.net/projects/jdk/)
* [Docker Engine](https://docs.docker.com/engine/install/) or [Docker Desktop](https://docs.docker.com/desktop/) 1.9 or later
* [Apache Maven](https://maven.apache.org/index.html) 3.9.8 or later  
  (or invoke the wrapper with `./mvnw` for Maven commands)

See the links above for installation instructions on your platform. You can verify the versions are installed and running:

    $ git --version
    $ javac -version
    $ mvn -version
    $ docker --version

### Why Docker?

Many open source software projects use Git, Java, and Maven, but requiring Docker is less common. Debezium is designed to talk to a number of external systems, such as various databases and services, and our integration tests verify Debezium does this correctly. But rather than expect you have all of these software systems installed locally, Debezium's build system uses Docker to automatically download or create the necessary images and start containers for each of the systems. The integration tests can then use these services and verify Debezium behaves as expected, and when the integration tests finish, Debezium's build will automatically stop any containers that it started.

Debezium also has a few modules that are not written in Java, and so they have to be required on the target operating system. Docker lets our build do this using images with the target operating system(s) and all necessary development tools.

Using Docker has several advantages:

1. You don't have to install, configure, and run specific versions of each external services on your local machine, or have access to them on your local network. Even if you do, Debezium's build won't use them.
1. We can test multiple versions of an external service. Each module can start whatever containers it needs, so different modules can easily use different versions of the services.
1. Everyone can run complete builds locally. You don't have to rely upon a remote continuous integration server running the build in an environment set up with all the required services.
1. All builds are consistent. When multiple developers each build the same codebase, they should see exactly the same results -- as long as they're using the same or equivalent JDK, Maven, and Docker versions. That's because the containers will be running the same versions of the services on the same operating systems. Plus, all of the tests are designed to connect to the systems running in the containers, so nobody has to fiddle with connection properties or custom configurations specific to their local environments.
1. No need to clean up the services, even if those services modify and store data locally. Docker *images* are cached, so reusing them to start containers is fast and consistent. However, Docker *containers* are never reused: they always start in their pristine initial state, and are discarded when they are shutdown. Integration tests rely upon containers, and so cleanup is handled automatically.

### Configure your Docker environment

The Docker Maven Plugin will resolve the docker host by checking the following environment variables:

    export DOCKER_HOST=tcp://10.1.2.2:2376
    export DOCKER_CERT_PATH=/path/to/cdk/.vagrant/machines/default/virtualbox/.docker
    export DOCKER_TLS_VERIFY=1

These can be set automatically if using Docker Machine or something similar.

#### Colima
In order to run testcontainers against [colima](https://github.com/abiosoft/colima) the env vars below should be set (assume we use `default` profile of colima)

    colima start
    export TESTCONTAINERS_DOCKER_SOCKET_OVERRIDE=/var/run/docker.sock
    export TESTCONTAINERS_HOST_OVERRIDE="0.0.0.0"
    export DOCKER_HOST="unix://${HOME}/.colima/default/docker.sock"


### Building the code

First obtain the code by cloning the Git repository:

    $ git clone https://github.com/debezium/debezium.git
    $ cd debezium

Then build the code using Maven:

    $ mvn clean verify

The build starts and uses several Docker containers for different DBMSes. Note that if Docker is not running or configured, you'll likely get an arcane error -- if this is the case, always verify that Docker is running, perhaps by using `docker ps` to list the running containers.

### Don't have Docker running locally for builds?

You can skip the integration tests and docker-builds with the following command:

    $ mvn clean verify -DskipITs

### Building just the artifacts, without running tests, CheckStyle, etc.

You can skip all non-essential plug-ins (tests, integration tests, CheckStyle, formatter, API compatibility check, etc.) using the "quick" build profile:

    $ mvn clean verify -Dquick

This provides the fastest way for solely producing the output artifacts, without running any of the QA related Maven plug-ins.
This comes in handy for producing connector JARs and/or archives as quickly as possible, e.g. for manual testing in Kafka Connect.

### Running tests of the Postgres connector using the wal2json or pgoutput logical decoding plug-ins

The Postgres connector supports three logical decoding plug-ins for streaming changes from the DB server to the connector: decoderbufs (the default), wal2json, and pgoutput.
To run the integration tests of the PG connector using wal2json, enable the "wal2json-decoder" build profile:

    $ mvn clean install -pl :debezium-connector-postgres -Pwal2json-decoder
    
To run the integration tests of the PG connector using pgoutput, enable the "pgoutput-decoder" and "postgres-10" build profiles:

    $ mvn clean install -pl :debezium-connector-postgres -Ppgoutput-decoder,postgres-10

A few tests currently don't pass when using the wal2json plug-in.
Look for references to the types defined in `io.debezium.connector.postgresql.DecoderDifferences` to find these tests.

### Running tests of the Postgres connector with specific Apicurio Version
To run the tests of PG connector using wal2json or pgoutput logical decoding plug-ins with a specific version of Apicurio, a test property can be passed as:

    $ mvn clean install -pl debezium-connector-postgres -Pwal2json-decoder 
          -Ddebezium.test.apicurio.version=1.3.1.Final

In absence of the property the stable version of Apicurio will be fetched.

### Running tests of the Postgres connector against an external database, e.g. Amazon RDS
Please note if you want to test against a *non-RDS* cluster, this test requires `<your user>` to be a superuser with not only `replication` but permissions
to login to `all` databases in `pg_hba.conf`.  It also requires `postgis` packages to be available on the target server for some of the tests to pass.

    $ mvn clean install -pl debezium-connector-postgres -Pwal2json-decoder \
         -Ddocker.skip.build=true -Ddocker.skip.run=true -Dpostgres.host=<your PG host> \
         -Dpostgres.user=<your user> -Dpostgres.password=<your password> \
         -Ddebezium.test.records.waittime=10

Adjust the timeout value as needed.

See [PostgreSQL on Amazon RDS](debezium-connector-postgres/RDS.md) for details on setting up a database on RDS to test against.

### Running tests of the Oracle connector using Oracle XStream

    $ mvn clean install -pl debezium-connector-oracle -Poracle-xstream,oracle-tests -Dinstantclient.dir=<path-to-instantclient>

### Running tests of the Oracle connector with a non-CDB database

    $ mvn clean install -pl debezium-connector-oracle -Poracle-tests -Dinstantclient.dir=<path-to-instantclient> -Ddatabase.pdb.name=

### Running the tests for MongoDB with oplog capturing from an IDE

When running the test without maven, please make sure you pass the correct parameters to the execution. Look for the correct parameters in `.github/workflows/mongodb-oplog-workflow.yml` and
append them to the JVM execution parameters, prefixing them with `debezium.test`. As the execution will happen outside of the lifecycle execution, you need to start the MongoDB container manually
from the MongoDB connector directory

    $ mvn docker:start -B -am -Passembly -Dcheckstyle.skip=true -Dformat.skip=true -Drevapi.skip -Dcapture.mode=oplog -Dversion.mongo.server=3.6 -Dorg.slf4j.simpleLogger.log.org.apache.maven.cli.transfer.Slf4jMavenTransferListener=warn -Dmaven.wagon.http.pool=false -Dmaven.wagon.httpconnectionManager.ttlSeconds=120 -Dcapture.mode=oplog -Dmongo.server=3.6

The relevant portion of the line will look similar to the following:

    java -ea -Ddebezium.test.capture.mode=oplog -Ddebezium.test.version.mongo.server=3.6 -Djava.awt.headless=true -Dconnector.mongodb.members.auto.discover=false -Dconnector.mongodb.name=mongo1 -DskipLongRunningTests=true [...]

## Contributing

The Debezium community welcomes anyone that wants to help out in any way, whether that includes reporting problems, helping with documentation, or contributing code changes to fix bugs, add tests, or implement new features. See [this document](CONTRIBUTE.md) for details.

A big thank you to all the Debezium contributors!

<a href="https://github.com/debezium/debezium/graphs/contributors">
  <img src="https://contributors-img.web.app/image?repo=debezium/debezium" />
</a>

# Oracleå½’æ¡£æ—¥å¿—æ™ºèƒ½åˆ†æ®µå¤„ç†å™¨

## æ¦‚è¿°

Oracleå½’æ¡£æ—¥å¿—æ™ºèƒ½åˆ†æ®µå¤„ç†å™¨æ˜¯ä¸€ä¸ªé«˜æ€§èƒ½çš„Pythonå·¥å…·ï¼Œç”¨äºè§£å†³Oracleå½’æ¡£æ—¥å¿—å¹¶è¡Œå¤„ç†çš„æ€§èƒ½ç“¶é¢ˆã€‚é€šè¿‡æ™ºèƒ½åˆ†æ®µç®—æ³•ï¼Œå®ƒèƒ½å¤Ÿå°†å¤§æ‰¹é‡çš„å½’æ¡£æ—¥å¿—æ•°æ®åˆ†æˆå¤šä¸ªå‡åŒ€çš„æ®µï¼Œç„¶åå¹¶è¡Œå¤„ç†ï¼Œæ˜¾è‘—æé«˜å¤„ç†æ•ˆç‡ã€‚

## æ ¸å¿ƒç‰¹æ€§

### ğŸ¯ æ™ºèƒ½åˆ†æ®µç®—æ³•
- **è‡ªé€‚åº”åˆ†æ®µ**: åŸºäºSCNèŒƒå›´å†…è®°å½•å¯†åº¦çš„é‡‡æ ·ä¼°ç®—ï¼ŒåŠ¨æ€è°ƒæ•´åˆ†æ®µç­–ç•¥
- **è´Ÿè½½å‡è¡¡**: ç¡®ä¿æ¯ä¸ªåˆ†æ®µçš„è®°å½•æ•°å°½å¯èƒ½æ¥è¿‘ç›®æ ‡å€¼ï¼Œé¿å…è´Ÿè½½ä¸å‡
- **äºŒåˆ†æŸ¥æ‰¾ä¼˜åŒ–**: ä½¿ç”¨äºŒåˆ†æŸ¥æ‰¾ç®—æ³•ç²¾ç¡®å®šä½åˆ†æ®µè¾¹ç•Œ

### ğŸš€ é«˜æ€§èƒ½å¹¶è¡Œå¤„ç†
- **å¤šçº¿ç¨‹å¤„ç†**: æ”¯æŒå¤šçº¿ç¨‹å¹¶è¡Œå¤„ç†å„ä¸ªåˆ†æ®µ
- **æ‰¹é‡å¤„ç†**: å†…ç½®æ‰¹é‡å¤„ç†æœºåˆ¶ï¼Œå‡å°‘I/Oå¼€é”€
- **è¿æ¥æ± **: æ”¯æŒæ•°æ®åº“è¿æ¥æ± ï¼Œæé«˜è¿æ¥å¤ç”¨ç‡

### ğŸ“Š æ™ºèƒ½ç›‘æ§ä¸ç»Ÿè®¡
- **å®æ—¶ç»Ÿè®¡**: æä¾›è¯¦ç»†çš„å¤„ç†ç»Ÿè®¡ä¿¡æ¯å’Œæ€§èƒ½æŒ‡æ ‡
- **å‡†ç¡®æ€§ç›‘æ§**: è·Ÿè¸ªåˆ†æ®µä¼°ç®—çš„å‡†ç¡®æ€§ï¼ŒæŒç»­ä¼˜åŒ–ç®—æ³•
- **è¿›åº¦ç›‘æ§**: å®æ—¶æ˜¾ç¤ºå¤„ç†è¿›åº¦å’Œå„åˆ†æ®µçŠ¶æ€

## å®‰è£…

### ç¯å¢ƒè¦æ±‚
- Python 3.7+
- Oracle Database 11g+ (æ”¯æŒLogMiner)
- Oracle Instant Client

### å®‰è£…æ­¥éª¤

1. å®‰è£…Pythonä¾èµ–:
```bash
pip install -r requirements.txt
```

2. é…ç½®Oracle Instant Client (æ ¹æ®æ‚¨çš„æ“ä½œç³»ç»Ÿ):
```bash
# Linux/macOS
export LD_LIBRARY_PATH=/path/to/instantclient:$LD_LIBRARY_PATH

# Windows
set PATH=C:\path\to\instantclient;%PATH%
```

3. ç¡®ä¿Oracleç”¨æˆ·å…·æœ‰LogMineræƒé™:
```sql
GRANT EXECUTE ON DBMS_LOGMNR TO your_user;
GRANT EXECUTE ON DBMS_LOGMNR_D TO your_user;
GRANT SELECT ON V_$LOGMNR_CONTENTS TO your_user;
GRANT SELECT ON V_$LOGMNR_LOGS TO your_user;
```

## å¿«é€Ÿå¼€å§‹

### åŸºç¡€ä½¿ç”¨

```python
from oracle_log_segmentation import (
    OracleLogSegmentProcessor, 
    SegmentationConfig
)
import cx_Oracle

# 1. é…ç½®æ•°æ®åº“è¿æ¥
def create_connection():
    return cx_Oracle.connect(
        user='your_user',
        password='your_password',
        dsn='localhost:1521/ORCL'
    )

# 2. é…ç½®LogMinerè®¾ç½®
def setup_logminer(archive_log_file):
    return f"""
    BEGIN
        DBMS_LOGMNR.ADD_LOGFILE(
            LOGFILENAME => '{archive_log_file}',
            OPTIONS => DBMS_LOGMNR.NEW
        );
        DBMS_LOGMNR.START_LOGMNR(
            OPTIONS => DBMS_LOGMNR.SKIP_CORRUPTION
        );
    END;
    """

# 3. å®šä¹‰è®°å½•å¤„ç†å‡½æ•°
def process_records(records):
    for record in records:
        print(f"å¤„ç†æ“ä½œ: {record['operation']} on {record['table_name']}")
        # åœ¨è¿™é‡Œå®ç°æ‚¨çš„ä¸šåŠ¡é€»è¾‘

# 4. åˆ›å»ºå¹¶ä½¿ç”¨å¤„ç†å™¨
config = SegmentationConfig(
    target_records_per_segment=100000,  # æ¯æ®µ10ä¸‡æ¡è®°å½•
    max_segments=8,                     # æœ€å¤š8ä¸ªåˆ†æ®µ
    sampling_ratio=0.001                # 0.1%é‡‡æ ·ç‡
)

processor = OracleLogSegmentProcessor(
    connection_factory=create_connection,
    logminer_setup_sql=setup_logminer('/path/to/archive.log'),
    config=config
)

# 5. å¹¶è¡Œå¤„ç†å½’æ¡£æ—¥å¿—
stats = processor.process_archive_log_parallel(
    start_scn=1000,
    end_scn=100000,
    record_processor=process_records,
    max_workers=4
)

print(f"å¤„ç†å®Œæˆ: {stats['total_records']}æ¡è®°å½•ï¼Œè€—æ—¶{stats['total_time']:.2f}ç§’")
```

## é…ç½®å‚æ•°è¯¦è§£

### SegmentationConfig

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `target_records_per_segment` | int | 100000 | æ¯ä¸ªåˆ†æ®µçš„ç›®æ ‡è®°å½•æ•° |
| `max_segments` | int | 20 | æœ€å¤§åˆ†æ®µæ•°é™åˆ¶ |
| `min_segments` | int | 2 | æœ€å°åˆ†æ®µæ•°é™åˆ¶ |
| `sampling_ratio` | float | 0.001 | é‡‡æ ·æ¯”ä¾‹ï¼ˆ0.1%ï¼‰ |
| `max_sampling_records` | int | 10000 | æœ€å¤§é‡‡æ ·è®°å½•æ•° |
| `tolerance_ratio` | float | 0.3 | å®¹å·®æ¯”ä¾‹ï¼ˆ30%ï¼‰ |

### æ€§èƒ½è°ƒä¼˜å»ºè®®

#### 1. åˆ†æ®µç­–ç•¥ä¼˜åŒ–
```python
# å¤§æ•°æ®é‡åœºæ™¯ï¼ˆç™¾ä¸‡çº§è®°å½•ï¼‰
config = SegmentationConfig(
    target_records_per_segment=200000,  # å¢å¤§åˆ†æ®µå¤§å°
    max_segments=16,                    # å¢åŠ æœ€å¤§åˆ†æ®µæ•°
    sampling_ratio=0.0005,              # é™ä½é‡‡æ ·ç‡å‡å°‘å¼€é”€
    tolerance_ratio=0.4                 # æ”¾å®½å®¹å·®æé«˜åˆ†æ®µé€Ÿåº¦
)

# å°æ•°æ®é‡åœºæ™¯ï¼ˆåä¸‡çº§è®°å½•ï¼‰
config = SegmentationConfig(
    target_records_per_segment=50000,   # å‡å°åˆ†æ®µå¤§å°
    max_segments=6,                     # å‡å°‘åˆ†æ®µæ•°
    sampling_ratio=0.002,               # æé«˜é‡‡æ ·ç‡ä¿è¯å‡†ç¡®æ€§
    tolerance_ratio=0.2                 # ä¸¥æ ¼æ§åˆ¶åˆ†æ®µå‡åŒ€æ€§
)
```

#### 2. å¹¶è¡Œåº¦é…ç½®
```python
import os

# æ ¹æ®CPUæ ¸å¿ƒæ•°è‡ªåŠ¨é…ç½®
max_workers = min(os.cpu_count(), len(segments))

# è€ƒè™‘æ•°æ®åº“è¿æ¥æ•°é™åˆ¶
max_workers = min(max_workers, 8)  # é€šå¸¸ä¸è¶…è¿‡8ä¸ªå¹¶å‘è¿æ¥
```

#### 3. å†…å­˜ä¼˜åŒ–
```python
def memory_efficient_processor(records):
    # æµå¼å¤„ç†ï¼Œé¿å…å¤§é‡æ•°æ®åœ¨å†…å­˜ä¸­ç§¯ç´¯
    for record in records:
        # ç«‹å³å¤„ç†å¹¶é‡Šæ”¾
        process_single_record(record)
        
    # å®šæœŸè§¦å‘åƒåœ¾å›æ”¶
    if len(records) > 10000:
        import gc
        gc.collect()
```

## é«˜çº§ç”¨æ³•

### 1. è‡ªå®šä¹‰åˆ†æ®µç­–ç•¥

```python
class CustomSegmentationAlgorithm(AdaptiveSegmentationAlgorithm):
    def create_segments(self, start_scn, end_scn):
        # å®ç°æ‚¨çš„è‡ªå®šä¹‰åˆ†æ®µé€»è¾‘
        # ä¾‹å¦‚ï¼šåŸºäºæ—¶é—´çª—å£åˆ†æ®µã€åŸºäºè¡¨ååˆ†æ®µç­‰
        pass
```

### 2. é”™è¯¯å¤„ç†å’Œé‡è¯•

```python
def robust_processor(records):
    max_retries = 3
    for attempt in range(max_retries):
        try:
            # å¤„ç†è®°å½•
            process_records_batch(records)
            break
        except Exception as e:
            if attempt == max_retries - 1:
                logger.error(f"å¤„ç†å¤±è´¥ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°: {e}")
                raise
            logger.warning(f"å¤„ç†å¤±è´¥ï¼Œæ­£åœ¨é‡è¯• ({attempt + 1}/{max_retries}): {e}")
            time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
```

### 3. ç»“æœæŒä¹…åŒ–

```python
def persistent_processor(records):
    # æ‰¹é‡å†™å…¥æ•°æ®åº“
    connection = get_target_connection()
    cursor = connection.cursor()
    
    batch_data = []
    for record in records:
        batch_data.append((
            record['scn'],
            record['timestamp'],
            record['operation'],
            record['table_name'],
            record['sql_redo']
        ))
    
    cursor.executemany("""
        INSERT INTO processed_changes 
        (scn, timestamp, operation, table_name, sql_redo)
        VALUES (?, ?, ?, ?, ?)
    """, batch_data)
    
    connection.commit()
    cursor.close()
```

## ç›‘æ§å’Œå‘Šè­¦

### 1. æ€§èƒ½ç›‘æ§

```python
def monitoring_processor(records):
    start_time = time.time()
    
    # å¤„ç†è®°å½•
    process_records(records)
    
    processing_time = time.time() - start_time
    records_per_second = len(records) / processing_time
    
    # æ€§èƒ½å‘Šè­¦
    if records_per_second < 1000:  # ä½äº1000æ¡/ç§’
        logger.warning(f"å¤„ç†é€Ÿåº¦è¾ƒæ…¢: {records_per_second:.1f} æ¡/ç§’")
    
    # è®°å½•æ€§èƒ½æŒ‡æ ‡
    metrics.gauge('processing.records_per_second', records_per_second)
    metrics.gauge('processing.batch_size', len(records))
```

### 2. æ•°æ®è´¨é‡ç›‘æ§

```python
def quality_monitoring_processor(records):
    error_count = 0
    warning_count = 0
    
    for record in records:
        # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
        if not record.get('scn'):
            error_count += 1
            logger.error(f"è®°å½•ç¼ºå°‘SCN: {record}")
            continue
            
        # æ£€æŸ¥å¯ç–‘æ“ä½œ
        if record['operation'] == 'DELETE' and 'CRITICAL' in record['table_name']:
            warning_count += 1
            logger.warning(f"å…³é”®è¡¨åˆ é™¤æ“ä½œ: {record['table_name']} at SCN {record['scn']}")
    
    # è´¨é‡æŠ¥å‘Š
    total_records = len(records)
    error_rate = error_count / total_records if total_records > 0 else 0
    
    if error_rate > 0.01:  # é”™è¯¯ç‡è¶…è¿‡1%
        logger.error(f"æ•°æ®è´¨é‡å‘Šè­¦: é”™è¯¯ç‡ {error_rate:.2%}")
```

## å¸¸è§é—®é¢˜è§£å†³

### Q1: åˆ†æ®µä¸å‡åŒ€æ€ä¹ˆåŠï¼Ÿ
**A**: è°ƒæ•´é‡‡æ ·å‚æ•°ï¼š
- å¢åŠ  `sampling_ratio` æé«˜é‡‡æ ·å‡†ç¡®æ€§
- å‡å°‘ `tolerance_ratio` ä¸¥æ ¼æ§åˆ¶åˆ†æ®µå‡åŒ€æ€§
- å¢åŠ é‡‡æ ·ç‚¹æ•°é‡ï¼ˆåœ¨ä»£ç ä¸­ä¿®æ”¹ `sample_points`ï¼‰

### Q2: å¤„ç†é€Ÿåº¦æ…¢æ€ä¹ˆåŠï¼Ÿ
**A**: æ€§èƒ½ä¼˜åŒ–ç­–ç•¥ï¼š
- å¢åŠ  `max_workers` æé«˜å¹¶è¡Œåº¦
- å‡å°‘ `sampling_ratio` é™ä½é‡‡æ ·å¼€é”€
- ä¼˜åŒ–è®°å½•å¤„ç†å‡½æ•°ï¼Œé¿å…å¤æ‚è®¡ç®—
- ä½¿ç”¨æ‰¹é‡å¤„ç†å’Œè¿æ¥æ± 

### Q3: å†…å­˜å ç”¨è¿‡é«˜æ€ä¹ˆåŠï¼Ÿ
**A**: å†…å­˜ä¼˜åŒ–æªæ–½ï¼š
- å‡å°‘ `target_records_per_segment` é™ä½å•æ®µå†…å­˜å ç”¨
- åœ¨å¤„ç†å‡½æ•°ä¸­åŠæ—¶é‡Šæ”¾å¤§å¯¹è±¡
- ä½¿ç”¨æµå¼å¤„ç†é¿å…æ•°æ®ç§¯ç´¯
- å®šæœŸè°ƒç”¨ `gc.collect()`

### Q4: LogMineræƒé™ä¸è¶³æ€ä¹ˆåŠï¼Ÿ
**A**: æƒé™é…ç½®ï¼š
```sql
-- ä»¥DBAèº«ä»½æ‰§è¡Œ
GRANT EXECUTE ON DBMS_LOGMNR TO your_user;
GRANT EXECUTE ON DBMS_LOGMNR_D TO your_user;
GRANT SELECT ON V_$LOGMNR_CONTENTS TO your_user;
GRANT SELECT ON V_$LOGMNR_LOGS TO your_user;
GRANT SELECT ON V_$ARCHIVED_LOG TO your_user;
```

## æœ€ä½³å®è·µ

1. **é¢„åˆ†æ**: å¤„ç†å‰å…ˆåˆ†æå½’æ¡£æ—¥å¿—çš„SCNèŒƒå›´å’Œè®°å½•åˆ†å¸ƒ
2. **æµ‹è¯•éªŒè¯**: åœ¨å°èŒƒå›´æ•°æ®ä¸Šæµ‹è¯•åˆ†æ®µæ•ˆæœ
3. **ç›‘æ§è°ƒä¼˜**: æŒç»­ç›‘æ§å¤„ç†æ€§èƒ½ï¼Œæ ¹æ®å®é™…æƒ…å†µè°ƒæ•´å‚æ•°
4. **é”™è¯¯å¤„ç†**: å®ç°å®Œå–„çš„é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶
5. **èµ„æºç®¡ç†**: åˆç†æ§åˆ¶å¹¶å‘æ•°ï¼Œé¿å…å¯¹æ•°æ®åº“é€ æˆè¿‡å¤§å‹åŠ›

## è®¸å¯è¯

MIT License

## è´¡çŒ®æŒ‡å—

æ¬¢è¿æäº¤Issueå’ŒPull Requestæ¥æ”¹è¿›è¿™ä¸ªé¡¹ç›®ï¼

## è”ç³»æ”¯æŒ

å¦‚æœæ‚¨åœ¨ä½¿ç”¨è¿‡ç¨‹ä¸­é‡åˆ°é—®é¢˜ï¼Œè¯·ï¼š
1. æŸ¥çœ‹å¸¸è§é—®é¢˜è§£å†³éƒ¨åˆ†
2. æäº¤Issueæè¿°å…·ä½“é—®é¢˜
3. æä¾›é”™è¯¯æ—¥å¿—å’Œé…ç½®ä¿¡æ¯
